#!/usr/bin/env python

import combine
import feeds
from scrapy.crawler import CrawlerProcess
from scrapy.settings import Settings
import persistent
import settings
from tornado.platform.twisted import TwistedIOLoop
import tornado.web


TwistedIOLoop().install()

full_settings = Settings()
full_settings.setmodule(settings)

process = CrawlerProcess(full_settings)
crawler = persistent.Crawler(full_settings)
process.crawl(crawler)


class FeedHandler(tornado.web.RequestHandler):
    @tornado.gen.coroutine
    def get(self, feed):
        entries = yield feeds.full_history(crawler, feed)
        self.set_header("Content-Type", "text/plain")
        for e in entries:
            self.write("{}\t{}\n".format(e["source"], e["id"]))
        self.finish()


application = tornado.web.Application(
    [
        (r"/history/(.+)", FeedHandler),
        (r"/read/(.+)", combine.ExportHandler, {"crawler": crawler}),
        (r"/entry/([^/]+)/(.+)", combine.EntryHandler, {"crawler": crawler}, "entry"),
    ],
    static_path="static/",
)
application.listen(8888)

process.start()
